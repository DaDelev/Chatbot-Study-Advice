{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain Chatbot (scaled up document base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import UnstructuredHTMLLoader\n",
    "from langchain.document_loaders import UnstructuredPDFLoader\n",
    "from langchain import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain\n",
    "langchain.debug = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load data and setup vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = []\n",
    "for file in os.listdir('data/scraped_data'):\n",
    "    if file.endswith('.pdf'):\n",
    "        pdf_path = './data/scraped_data/' + file\n",
    "        print(f'Loading {pdf_path}')\n",
    "        loader = UnstructuredPDFLoader(pdf_path)\n",
    "        documents.extend(loader.load())\n",
    "    elif file.endswith('.html'):\n",
    "        doc_path = './data/scraped_data/' + file\n",
    "        print(f'Loading {doc_path}')\n",
    "        loader = UnstructuredHTMLLoader(doc_path)\n",
    "        documents.extend(loader.load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split documents into text chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "chunked_documents = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create chroma vector db with OpenAIEmbeddings\n",
    "persist_directory = './storage_scaled_up'\n",
    "\n",
    "if not os.listdir(persist_directory):\n",
    "\n",
    "    vectordb = Chroma.from_documents(\n",
    "      chunked_documents,\n",
    "      embedding=OpenAIEmbeddings(),\n",
    "      persist_directory=persist_directory\n",
    "    )\n",
    "\n",
    "    vectordb.persist()\n",
    "\n",
    "else:\n",
    "    vectordb = Chroma(persist_directory=persist_directory, embedding_function=OpenAIEmbeddings())\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create QA chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Prompt\n",
    "template = \"\"\"\n",
    "\n",
    "If the question does not contain a study program, say that you need more information about the study program to answer the question.\n",
    "\n",
    "Use the following pieces of context to answer the question at the end.\n",
    "\n",
    "Execute these steps:\n",
    "1 - list the context\n",
    "2 - focus on words like \"optional\" or \"can\" for your answer\n",
    "3 - answer the question. Do not use information outside of the context to answer the question.\n",
    "\n",
    "Your answer should have this format:\n",
    "\n",
    "context:\n",
    "answer:\n",
    "\n",
    "------------------------\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "custom_prompt = PromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=ChatOpenAI(model='gpt-3.5-turbo', temperature=0),\n",
    "    retriever=vectordb.as_retriever(search_kwargs={'k': 5}),\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": custom_prompt}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests\n",
    "### read questions and answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_questions = pd.read_csv('TestQuestions.csv', delimiter=\";\", names=[\"Question\", \"Response\"] )\n",
    "questions = df_questions[\"Question\"]\n",
    "\n",
    "responses = []\n",
    "counter = 0\n",
    "\n",
    "for q in questions:\n",
    "    print(f'q{counter} start')\n",
    "    \n",
    "    # get result\n",
    "    result_object = qa_chain({'query': q})\n",
    "    r = result_object['result']\n",
    "    \n",
    "    # get source documents\n",
    "    source_docs = result_object['source_documents']\n",
    "    sources = []\n",
    "    for doc in source_docs:\n",
    "        sources.append(doc.metadata[\"source\"].replace('./data/scraped_data/', ''))\n",
    "    source = \",\".join(sources)\n",
    "    \n",
    "    # build row\n",
    "    responses.append((q, r, source))\n",
    "    \n",
    "    print(f'q{counter} end')\n",
    "    counter += 1\n",
    "\n",
    "df_responses = pd.DataFrame(responses, columns=[\"Question\", \"Response\", \"Source\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_responses.to_csv(\"test_responses_scaled_w_source.csv\", sep=\";\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
