{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain Chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "import os\n",
    "os.environ['OPENAI_API_KEY'] = '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'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import UnstructuredHTMLLoader\n",
    "from langchain.document_loaders import UnstructuredPDFLoader\n",
    "# from langchain_community.document_loaders import UnstructuredPDFLoader\n",
    "# from langchain.document_loaders import PyPDFLoader\n",
    "from langchain import PromptTemplate\n",
    "import pytesseract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain\n",
    "langchain.debug = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pytesseract.pytesseract.tesseract_cmd = r'C:\\Program Files\\Tesseract-OCR\\tesseract.exe'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load data and setup vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = []\n",
    "for file in os.listdir('data/scraped_data'):\n",
    "    if file.endswith('.pdf'):\n",
    "        pdf_path = './data/scraped_data/' + file\n",
    "        print(f'Loading {pdf_path}')\n",
    "        loader = UnstructuredPDFLoader(pdf_path)\n",
    "        documents.extend(loader.load())\n",
    "    elif file.endswith('.html'):\n",
    "        doc_path = './data/scraped_data/' + file\n",
    "        print(f'Loading {doc_path}')\n",
    "        loader = UnstructuredHTMLLoader(doc_path)\n",
    "        documents.extend(loader.load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split documents into text chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "chunked_documents = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\danie\\anaconda3\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.embeddings.openai.OpenAIEmbeddings` was deprecated in langchain-community 0.1.0 and will be removed in 0.2.0. Use langchain_openai.OpenAIEmbeddings instead.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "# create chroma vector db with OpenAIEmbeddings\n",
    "persist_directory = './storage_scaled_up'\n",
    "\n",
    "if not os.listdir(persist_directory):\n",
    "\n",
    "    vectordb = Chroma.from_documents(\n",
    "      chunked_documents,\n",
    "      embedding=OpenAIEmbeddings(),\n",
    "      persist_directory=persist_directory\n",
    "    )\n",
    "\n",
    "    vectordb.persist()\n",
    "\n",
    "else:\n",
    "    vectordb = Chroma(persist_directory=persist_directory, embedding_function=OpenAIEmbeddings())\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create QA chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Prompt\n",
    "template = \"\"\"\n",
    "\n",
    "If the question does not contain a study program, say that you need more information about the study program to answer the question.\n",
    "\n",
    "Use the following pieces of context to answer the question at the end. \n",
    "\n",
    "Execute these steps:\n",
    "1 - list the context \n",
    "2 - focus on words like \"optional\" or \"can\" for your answer\n",
    "3 - answer the question. Do not use information outside of the context to answer the question\n",
    "\n",
    "Your answer should have this format:\n",
    "\n",
    "context:\n",
    "answer:\n",
    "------------------------\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "custom_prompt = PromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Prompt\n",
    "template = \"\"\"\n",
    "\n",
    "If the question does not contain a study program, say that you need more information about the study program to answer the question.\n",
    "\n",
    "Use the following pieces of context to answer the question at the end.\n",
    "\n",
    "Execute these steps:\n",
    "1 - list the context\n",
    "2 - focus on words like \"optional\" or \"can\" for your answer\n",
    "3 - answer the question. Do not use information outside of the context to answer the question.\n",
    "\n",
    "Your answer should have this format:\n",
    "\n",
    "context:\n",
    "answer:\n",
    "\n",
    "------------------------\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "custom_prompt = PromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=ChatOpenAI(model='gpt-3.5-turbo', temperature=0),\n",
    "    retriever=vectordb.as_retriever(search_kwargs={'k': 5}),\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": custom_prompt}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests\n",
    "### read questions and answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain_community.embeddings.openai.embed_with_retry.<locals>._embed_with_retry in 4.0 seconds as it raised ServiceUnavailableError: The server is overloaded or not ready yet..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context: This degree program provides students with a solid theoretical foundation as well as practical skills for data management, data analytics methods and responsible data science. The courses are divided into two groups – fundamental courses and advanced courses. After studying optional fundamental courses in computer science and empirical social sciences, in their advanced courses students can focus on the concepts and methods of computers science and advanced empirical methods and the application of these methods. In addition to the regular lecture courses, students participate in a one or two semester team project or individual project.\n",
      "\n",
      "During their studies -\n",
      "\n",
      "(MK1) all students develop a deep understanding of the relevant concepts, methods and problem-solving strategies used in different application domains.\n",
      "\n",
      "(MK2) technology-oriented students learn the concepts, algorithms and strategies\n",
      "\n",
      "Answer: Yes, as a student studying the master of data science, you have the option to study fundamental courses in computer science and empirical social sciences.\n"
     ]
    }
   ],
   "source": [
    "q = \"I am studying the master of data science, do I have to do any fundamental courses?\"\n",
    "\n",
    "r = qa_chain({'query': q})\n",
    "print(r['result'])\n",
    "\n",
    "# print('\\nSources:')\n",
    "# for source_doc in r['source_documents']:\n",
    "#     print(source_doc)\n",
    "#     print('====================\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = \"Ich studiere den Bachelor in Wirtschaftsinformatik. Brauche ich Schlüsselqualifikationen?\"\n",
    "\n",
    "r = qa_chain({'query': q})\n",
    "print(r['result'])\n",
    "\n",
    "# print('\\nSources:')\n",
    "# for source_doc in r['source_documents']:\n",
    "#     print(source_doc)\n",
    "#     print('====================\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = \"Ich studiere Wirtschafstmathematik. Brauche ich Schlüsselqualifikationen?\"\n",
    "\n",
    "r = qa_chain({'query': q})\n",
    "print(r['result'])\n",
    "\n",
    "# print('\\nSources:')\n",
    "# for source_doc in r['source_documents']:\n",
    "#     print(source_doc)\n",
    "#     print('====================\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = \"I am studying the master of business informatics, do I have to do any fundamental courses?\"\n",
    "\n",
    "r = qa_chain({'query': q})\n",
    "print(r['result'])\n",
    "\n",
    "# print('\\nSources:')\n",
    "# for source_doc in r['source_documents']:\n",
    "#     print(source_doc)\n",
    "#     print('====================\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_questions = pd.read_csv('TestQuestions.csv', delimiter=\";\", names=[\"Question\", \"Response\"] )\n",
    "questions = df_questions[\"Question\"]\n",
    "\n",
    "responses = []\n",
    "counter = 0\n",
    "\n",
    "for q in questions:\n",
    "    print(f'q{counter} start')\n",
    "    \n",
    "    # get result\n",
    "    result_object = qa_chain({'query': q})\n",
    "    r = result_object['result']\n",
    "    \n",
    "    # get source documents\n",
    "    source_docs = result_object['source_documents']\n",
    "    sources = []\n",
    "    for doc in source_docs:\n",
    "        sources.append(doc.metadata[\"source\"].replace('./data/scraped_data/', ''))\n",
    "    source = \",\".join(sources)\n",
    "    \n",
    "    # build row\n",
    "    responses.append((q, r, source))\n",
    "    \n",
    "    print(f'q{counter} end')\n",
    "    counter += 1\n",
    "\n",
    "df_responses = pd.DataFrame(responses, columns=[\"Question\", \"Response\", \"Source\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_responses.to_csv(\"test_responses_scaled_w_source.csv\", sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
