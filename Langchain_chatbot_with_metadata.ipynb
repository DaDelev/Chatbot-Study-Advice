{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain Chatbot - with Additional LLM-generated Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from functools import partial\n",
    "from copy import deepcopy\n",
    "\n",
    "from langchain.document_loaders import UnstructuredHTMLLoader, UnstructuredPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_community.embeddings import OpenAIEmbeddings\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chains.combine_documents import collapse_docs, split_list_of_docs\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_transformers.openai_functions import create_metadata_tagger\n",
    "from langchain.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain.schema import Document, StrOutputParser\n",
    "from langchain_core.prompts import format_document\n",
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "\n",
    "from openai.error import InvalidRequestError\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "_ = load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = []\n",
    "for file in os.listdir('data/test_documents'):\n",
    "    if file.endswith('.pdf'):\n",
    "        pdf_path = './data/test_documents/' + file\n",
    "        print(f'Loading {pdf_path}')\n",
    "        loader = UnstructuredPDFLoader(pdf_path)\n",
    "        documents.extend(loader.load())\n",
    "    elif file.endswith('.html'):\n",
    "        doc_path = './data/test_documents/' + file\n",
    "        print(f'Loading {doc_path}')\n",
    "        loader = UnstructuredHTMLLoader(doc_path)\n",
    "        documents.extend(loader.load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Metadata\n",
    "\n",
    "- source\n",
    "- tag every document either with **general** or with relevant **studyprograms**\n",
    "- (summary for documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0, model=\"gpt-3.5-turbo\")\n",
    "\n",
    "metadata_prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"Extract relevant information from the following document.\n",
    "The document is related to the University of Mannheim. Some documents are only relevant to a specific \\\n",
    "study program, while others provide general information about the University of several study \\\n",
    "programs at once (use the \"general\" tag). If you think the document is relevant to a specific study \\\n",
    "program which is not in the list use the \"other\" tag.\n",
    "\n",
    "{input}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "schema = {\n",
    "    \"properties\": {\n",
    "        \"study_program\": {\n",
    "            \"type\": \"string\",\n",
    "            \"enum\": [\n",
    "                \"B.Sc. Business Informatics\",\n",
    "                \"M.Sc. Business Informatics\",\n",
    "                \"B.Sc. Mathematics in Business and Economics\",\n",
    "                \"M.Sc. Mathematics in Business and Economics\",\n",
    "                \"Mannheim Master in Data Science\",\n",
    "                \"general\",\n",
    "                \"other\"\n",
    "            ],\n",
    "            \"description\": \"The study program this document is relevant to\"\n",
    "        },\n",
    "        \"short_description\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"A short summary that describes what information can be found in this document in at most 3 sentences\"\n",
    "        },\n",
    "    },\n",
    "    \"required\": [\"study_program\", \"short_description\"]\n",
    "}\n",
    "\n",
    "document_transformer = create_metadata_tagger(metadata_schema=schema, llm=llm, prompt=metadata_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt and method for converting Document -> str.\n",
    "document_prompt = PromptTemplate.from_template(\"{page_content}\")\n",
    "partial_format_document = partial(format_document, prompt=document_prompt)\n",
    "\n",
    "\n",
    "# A text splitter that recursively splits a document into multiple chunks until\n",
    "# the maximum chunck size is below a predefined value (without overlap).\n",
    "\n",
    "def get_num_tokens_single_doc(doc):\n",
    "    return llm.get_num_tokens(partial_format_document(doc))\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 3600,\n",
    "    chunk_overlap  = 0,\n",
    "    length_function = llm.get_num_tokens,\n",
    "    is_separator_regex = False,\n",
    ")\n",
    "\n",
    "\n",
    "# The chain we'll apply to each individual document.\n",
    "# Returns a summary of the document.\n",
    "\n",
    "map_chain = (\n",
    "    {\"context\": partial_format_document}\n",
    "    | PromptTemplate.from_template(\"Summarize this content:\\n\\n{context}\")\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# A wrapper chain to keep the original Document metadata\n",
    "map_as_doc_chain = (\n",
    "    RunnableParallel({\"doc\": RunnablePassthrough(), \"content\": map_chain})\n",
    "    | (lambda x: Document(page_content=x[\"content\"], metadata=x[\"doc\"].metadata))\n",
    ").with_config(run_name=\"Summarize (return doc)\")\n",
    "\n",
    "\n",
    "# The chain we'll repeatedly apply to collapse subsets of the documents\n",
    "# into a consolidate document until the total token size of our\n",
    "# documents is below some max size.\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(partial_format_document(doc) for doc in docs)\n",
    "\n",
    "collapse_chain = (\n",
    "    {\"context\": format_docs}\n",
    "    | PromptTemplate.from_template(\"Collapse this content:\\n\\n{context}\")\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "def get_num_tokens(docs):\n",
    "    return llm.get_num_tokens(format_docs(docs))\n",
    "\n",
    "def collapse(\n",
    "    docs,\n",
    "    config,\n",
    "    token_max=3600,\n",
    "):\n",
    "    collapse_ct = 1\n",
    "    while get_num_tokens(docs) > token_max:\n",
    "        config[\"run_name\"] = f\"Collapse {collapse_ct}\"\n",
    "        invoke = partial(collapse_chain.invoke, config=config)\n",
    "        split_docs = split_list_of_docs(docs, get_num_tokens, token_max)\n",
    "        docs = [collapse_docs(_docs, invoke) for _docs in split_docs]\n",
    "        collapse_ct += 1\n",
    "    return docs\n",
    "\n",
    "\n",
    "# The chain we'll use to combine our individual document summaries\n",
    "# (or summaries over subset of documents if we had to collapse the map results)\n",
    "# into a final summary.\n",
    "\n",
    "reduce_chain = (\n",
    "    {\"context\": format_docs}\n",
    "    | PromptTemplate.from_template(\"Combine these summaries:\\n\\n{context}\")\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ").with_config(run_name=\"Reduce\")\n",
    "\n",
    "\n",
    "# The final full chain for summarizing documents\n",
    "map_reduce_summarizer = (text_splitter.split_documents | map_as_doc_chain.map() | collapse | reduce_chain).with_config(\n",
    "    run_name=\"Map reduce\"\n",
    ")\n",
    "\n",
    "\n",
    "def get_metadata(docs, metadata_tagger, summarizer, llm, max_tokens):\n",
    "    document_prompt = PromptTemplate.from_template(\"{page_content}\")\n",
    "    partial_format_document = partial(format_document, prompt=document_prompt)\n",
    "    docs_transformed = []\n",
    "    for i, doc in enumerate(docs):\n",
    "        print(f\"Processing document {i}/{len(docs)-1}\")\n",
    "        # directly apply metadata tagger if number of tokens is below threshold\n",
    "        if llm.get_num_tokens(partial_format_document(doc)) <= max_tokens:\n",
    "            print(f\"\\tApply metadata tagger...\")\n",
    "            doc_w_metadata = metadata_tagger.transform_documents([doc])[0]\n",
    "            docs_transformed.append(doc_w_metadata)\n",
    "        # otherwise, summarize document first to get metadata\n",
    "        else:\n",
    "            print(f\"\\tSummarize document...\")\n",
    "            summary = summarizer.invoke([doc])\n",
    "            summary_doc = Document(page_content=summary, metadata=doc.metadata)\n",
    "            print(f\"\\tApply metadata tagger...\")\n",
    "            summary_w_metadata = metadata_tagger.transform_documents([summary_doc])[0]\n",
    "            doc_w_metadata = Document(page_content=doc.page_content, metadata=summary_w_metadata.metadata)\n",
    "            docs_transformed.append(doc_w_metadata)\n",
    "    print(\"done!\")\n",
    "    return docs_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_w_metadata = get_metadata(documents, metadata_tagger=document_transformer, summarizer=map_reduce_summarizer, llm=llm, max_tokens=3600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, d in enumerate(documents_w_metadata):\n",
    "    print(f\"### document {i} ###\")\n",
    "    for k, v in d.metadata.items():\n",
    "        print(f\"{k}: {v}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Documents and Create Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split documents into text chunks\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "chunked_documents = text_splitter.split_documents(documents_w_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create chroma vector db with OpenAIEmbeddings\n",
    "\n",
    "vectordb = Chroma.from_documents(\n",
    "  chunked_documents,\n",
    "  embedding=OpenAIEmbeddings(),\n",
    "  persist_directory='./storage_langchain_w_metadata'\n",
    ")\n",
    "vectordb.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=ChatOpenAI(model='gpt-3.5-turbo'),\n",
    "    retriever=vectordb.as_retriever(search_kwargs={'k': 5}),\n",
    "    return_source_documents=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Questions and Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_questions = pd.read_csv('TestQuestions.csv', delimiter=\";\", names=[\"Question\", \"Response\"] )\n",
    "questions = df_questions[\"Question\"]\n",
    "\n",
    "responses = []\n",
    "counter = 0\n",
    "for q in questions:\n",
    "    print(f'q{counter} start')\n",
    "    r = qa_chain({'query': q})['result']\n",
    "    responses.append((q, r))\n",
    "    print(f'q{counter} end')\n",
    "    counter += 1\n",
    "\n",
    "df_responses = pd.DataFrame(responses, columns=[\"Question\", \"Response\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_responses.to_csv(\"test_responses_by_langchain_w_metadata.csv\", sep=\";\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
