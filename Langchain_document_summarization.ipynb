{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Summarization with Huggingface LLMs\n",
    "\n",
    "- https://python.langchain.com/docs/integrations/llms/huggingface_pipelines\n",
    "- https://huggingface.co/models?pipeline_tag=summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.document_loaders import UnstructuredHTMLLoader, UnstructuredPDFLoader\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from functools import partial\n",
    "from langchain_core.prompts import format_document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chains.combine_documents import collapse_docs, split_list_of_docs\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_transformers.openai_functions import create_metadata_tagger\n",
    "from langchain.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain.schema import Document, StrOutputParser\n",
    "from langchain_core.prompts import format_document\n",
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = []\n",
    "for file in os.listdir('data/test_documents'):\n",
    "    if file.endswith('.pdf'):\n",
    "        pdf_path = './data/test_documents/' + file\n",
    "        print(f'Loading {pdf_path}')\n",
    "        loader = UnstructuredPDFLoader(pdf_path)\n",
    "        documents.extend(loader.load())\n",
    "    elif file.endswith('.html'):\n",
    "        doc_path = './data/test_documents/' + file\n",
    "        print(f'Loading {doc_path}')\n",
    "        loader = UnstructuredHTMLLoader(doc_path)\n",
    "        documents.extend(loader.load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = HuggingFacePipeline.from_model_id(\n",
    "    model_id=\"facebook/bart-large-cnn\",\n",
    "    task=\"summarization\",\n",
    "    pipeline_kwargs={\"max_new_tokens\": 100},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hf = HuggingFacePipeline.from_model_id(\n",
    "#    model_id=\"Falconsai/text_summarization\",\n",
    "#    task=\"summarization\",\n",
    "#    pipeline_kwargs={\"max_new_tokens\": 10},\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_prompt = PromptTemplate.from_template(\"{page_content}\")\n",
    "partial_format_document = partial(format_document, prompt=document_prompt)\n",
    "def get_num_tokens_single_doc(doc):\n",
    "    return llm.get_num_tokens(partial_format_document(doc))\n",
    "\n",
    "for doc in documents:\n",
    "    print(get_num_tokens_single_doc(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_summary(docs, llm, max_tokens):\n",
    "    # Prompt and method for converting Document -> str.\n",
    "    document_prompt = PromptTemplate.from_template(\"{page_content}\")\n",
    "    partial_format_document = partial(format_document, prompt=document_prompt)\n",
    "\n",
    "    # A text splitter that recursively splits a document into multiple chunks until\n",
    "    # the maximum chunck size is below a predefined value (without overlap).\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size = max_tokens,\n",
    "        chunk_overlap  = 0,\n",
    "        length_function = llm.get_num_tokens,\n",
    "        is_separator_regex = False,\n",
    "    )\n",
    "\n",
    "    # The chain we'll apply to each individual document.\n",
    "    # Returns a summary of the document.\n",
    "    map_chain = (\n",
    "        {\"context\": partial_format_document}\n",
    "        | PromptTemplate.from_template(\"Summarize this content:\\n\\n{context}\")\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "\n",
    "    # A wrapper chain to keep the original Document metadata\n",
    "    map_as_doc_chain = (\n",
    "        RunnableParallel({\"doc\": RunnablePassthrough(), \"content\": map_chain})\n",
    "        | (lambda x: Document(page_content=x[\"content\"], metadata=x[\"doc\"].metadata))\n",
    "    ).with_config(run_name=\"Summarize (return doc)\")\n",
    "\n",
    "\n",
    "    # The chain we'll repeatedly apply to collapse subsets of the documents\n",
    "    # into a consolidate document until the total token size of our\n",
    "    # documents is below some max size.\n",
    "    def format_docs(docs):\n",
    "        return \"\\n\\n\".join(partial_format_document(doc) for doc in docs)\n",
    "\n",
    "    collapse_chain = (\n",
    "        {\"context\": format_docs}\n",
    "        | PromptTemplate.from_template(\"Collapse this content:\\n\\n{context}\")\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "\n",
    "    def get_num_tokens(docs):\n",
    "        return llm.get_num_tokens(format_docs(docs))\n",
    "\n",
    "    def collapse(\n",
    "        docs,\n",
    "        config,\n",
    "        token_max=max_tokens,\n",
    "    ):\n",
    "        collapse_ct = 1\n",
    "        while get_num_tokens(docs) > token_max:\n",
    "            config[\"run_name\"] = f\"Collapse {collapse_ct}\"\n",
    "            invoke = partial(collapse_chain.invoke, config=config)\n",
    "            split_docs = split_list_of_docs(docs, get_num_tokens, token_max)\n",
    "            docs = [collapse_docs(_docs, invoke) for _docs in split_docs]\n",
    "            collapse_ct += 1\n",
    "        return docs\n",
    "\n",
    "    # The chain we'll use to combine our individual document summaries\n",
    "    # (or summaries over subset of documents if we had to collapse the map results)\n",
    "    # into a final summary.\n",
    "\n",
    "    reduce_chain = (\n",
    "        {\"context\": format_docs}\n",
    "        | PromptTemplate.from_template(\"Combine these summaries:\\n\\n{context}\")\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    ).with_config(run_name=\"Reduce\")\n",
    "\n",
    "\n",
    "    # The final full chain for summarizing documents\n",
    "    map_reduce_summarizer = (text_splitter.split_documents | map_as_doc_chain.map() | collapse | reduce_chain).with_config(\n",
    "        run_name=\"Map reduce\"\n",
    "    )\n",
    "\n",
    "    docs_summarized = []\n",
    "    for i, doc in enumerate(docs):\n",
    "        print(f\"Processing document {i}/{len(docs)-1}\")\n",
    "        summary = map_reduce_summarizer.invoke([doc])\n",
    "        summary_doc = Document(page_content=summary, metadata=doc.metadata)\n",
    "        docs_summarized.append(summary_doc)\n",
    "    print(\"done!\")\n",
    "\n",
    "    return docs_summarized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_summary_bart = get_summary(documents, llm=llm, max_tokens=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_summary_bart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm2 = ChatOpenAI(temperature=0, model=\"gpt-3.5-turbo-0125\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_summary_gpt_3_5 = get_summary(documents, llm=llm2, max_tokens=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_summary_gpt_3_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_summary_gpt_3_5[0].metadata[\"source\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summaries_df = pd.DataFrame({\n",
    "    \"doc\": [d.metadata[\"source\"] for d in doc_summary_gpt_3_5],\n",
    "    \"bart_summary\": [d.page_content for d in doc_summary_bart],\n",
    "    \"gpt_3_5_summary\": [d.page_content for d in doc_summary_gpt_3_5]\n",
    "})\n",
    "\n",
    "summaries_df.to_csv(\"data/summaries.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
